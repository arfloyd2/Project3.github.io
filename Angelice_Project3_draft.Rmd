---
title: "Angelice_Project3 Education Level: `r params$edlvl` "
author: "Angelice Floyd"
params:
  edlvl: 1
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r rndr, eval = FALSE, echo = FALSE}

 pwalk(c(1,3),rmarkdown::render(input = "Angelice_Project3_draft.Rmd",
              output_format = "github_document", 
              output_options = list(
                name_value_pairs = "value", 
                toc = TRUE,
                toc_depth = 4, 
                number_of_sections = TRUE, 
                df_print = "paged", 
                or_something = TRUE,
                params = lapply(c(1,3),function(x){list(edlvl = x)}),
                html_preview = FALSE,
                output_file = paste0("Project3","Education level:",params$edlvl,".html")
                
)
)
)
```


Introduction : Insert Michael's intro here 

```{r libraries}

library(tidyverse)
library(caret)
library(GGally)
library(shiny)
library(rmarkdown)
library(MASS)
```


# Reading in the Data 

Below reads in the binary heart disease data, converts the appropriate variables into factor level variables, and splits the data into training and test set data. 

```{r readin, message = FALSE}

params$edlvl

initial_dta <- read_csv('diabetes_binary_health_indicators_BRFSS2015.csv') %>% mutate( Education_1 = ifelse(Education %in% c(1,2),1,Education)) %>% filter(Education1 == 1 )


cols <- c("Diabetes_binary","PhysActivity","AnyHealthcare","Education1","HighBP",
          "Smoker","Fruits","NoDocbcCost","Income","DiffWalk","HighChol","Stroke",
          "Veggies","GenHlth","Sex","CholCheck","HeartDiseaseorAttack","HvyAlcoholConsump",
          "MentHlth")
  
initial_dta[cols] <- lapply(initial_dta[cols],factor)

#set seed for reproduceability 

set.seed(90)

#create a training index that uses 70% of the data for training data

trainindex <- createDataPartition(initial_dta$Diabetes_binary,p = 0.70, list= FALSE)

#Now, split the data into 70% training and 30% testing

initial_train <- initialdta[trainindex, ]
initial_test <- initialdta[-trainindex, ]

#To help make our predictions valid, we are now going to standardize the numeric variables

preprocval <- preProcess(initial_train,method = c("center","scale"))

trainTransformed <- as_tibble(predict(preprocval,initial_train))


testTransformed <- as_tibble(predict(preprocval,initial_test))

```

# Exploratory Data Analysis 

Because this data is in binary form, contingency tables and categorical visuals will be helpful since it will show how the different levels of those with and without diabetes compare to each other over levels of other categorical variables or how values for the numerical variables conpare at different levels for those with or without diabetes. 

## Diabetes Patients and Pre-Existing Conditions 

This frequency table will show the number of patients with and without diabetes who have( or do not have) high blood pressure, high cholestroral, have had stroke or have had a heart attack or heart disease.  

Might be a better way to combine these 

```{r healthconcerns}


BP <- data.frame(table(initial_dta$Diabetes_binary,initial_dta$HighBP)) %>% rename(Diabetes_binary="Var1", High_BP = "Var2", BP_Freq = "Freq")

CHOL <- data.frame(table(initial_dta$Diabetes_binary, initial_dta$HighChol)) %>% rename(Diabetes_binary="Var1", High_Chol = "Var2", HI_CHOL_Freq = "Freq" )

STRKE <- as.data.frame(table(initial_dta$Diabetes_binary, initial_dta$Stroke)) %>% rename(Diabetes_binary="Var1", Stroke = "Var2", sTROKE_Freq = "Freq" )


HATCK <- as.data.frame(table(initial_dta$Diabetes_binary, initial_dta$HeartDiseaseorAttack)) %>% rename(Diabetes_binary="Var1", Heart_Disease_Attack = "Var2", Heart_Disease_Attack_Freq = "Freq" )

Diabetes_disease_frequency <- list("BP" = BP,"CHOL" = CHOL,"STRKE" = STRKE,"HATCK" = HATCK)

Diabetes_disease_frequency

```

## The Case for Age 


One area of interest was to see how the variable age interacts with the binary diabetes. Overall, diabetes is associated with poeple of older ages, however, i found it interesting to see how distributed the ages are amongst the different educational levels. Is the data skewed towards the older ages for all educational categories, or is there a point in which the distribution is more evenly spread out, or even centered around the mean age ? before that happens, it would be which age groups tend to have better fitness and nutrition habits ( such as whether or not the subject has had physical activity in the past 30 days, do they consume fruit one or more times a day, and do they consume 1 or more vegetables per day).These healthy indicators could provide insight on the patterns for age groups of those who have diabetes. 



### Mean, Median, Max Age for Each Diabetes indicator 

This section will look at some levels of center

```{r diabage}
diabgroup <- initial_dta %>% summarise(Max_age = max(Age), Mediam_Age = median(Age), Mean_Age = mean(Age), .by = Diabetes_binary)

diabgroup
```

### Distribution of Age for patients with Diabetes 


```{r graphage}

diab_age <- initial_dta %>% filter(Diabetes_binary == 1)

ggplot(diabage) +
  #geom_histogram(mapping=aes(x=Age), position="identity", bins = 50) +
  geom_density(mapping=aes(x=Age ), fill = "blue" ,position="identity") + 
  labs(x = "Count of Age", title = "Distribution of Age for Subjects with Diabetes")

```

### Healthy Habits indicators broken out by First, Second, and Third quartile Age 

This section first creates a variable that breaks the ages into four groups. The first are ages below the first quartile ( group 1) . The second group consists of ages between the first and the third quartile ( group 2). The third group consistes of those above the 3rd quartile ( group 3). Since the Age variabel continains unique values per observation, creating age groups will be useful for the health habit summarizations. The fourth is a check to see if there are any values that do not fall within the aforementioned groups. 

```{r agegroup}

age1 <- quantile(initial_dta$Age, probs = 0.25)
age2 <- quantile(initial_dta$Age, probs = 0.5 )
age3 <- quantile(initial_dta$Age, probs = 0.75)

initial_age_group <- initial_dta

initial_age_group$AgeGroup <- ifelse(initial_age_group$Age< age1,1,
                                     ifelse(initial_age_group$Age >= age1 & initial_age_group$Age < age2,2,
                                            ifelse(initial_age_group$Age >= age2 & initial_age_group$Age < age3,3,
                                                   ifelse(initial_age_group$Age >= age3 ,4, 0))))

```

This next chunk  of code produces a series of contingency tables to see the frequency of healthy indicator values for each age group. 

later : observe for maybe better way to display the contingency tables 

```{r healthlist}

PHYS <- data.frame(table(initial_age_group$AgeGroup,initial_age_group$PhysActivity)) %>% rename(Age_Group="Var1", Physical_Acitivty = "Var2", Phys_Freq = "Freq")

FRUIT <- data.frame(table(initial_age_group$AgeGroup,initial_age_group$Fruits)) %>% rename(Age_Group="Var1", Consumes_Fruit = "Var2", Fruit_Consumption_Freq = "Freq")

VEG <- data.frame(table(initial_age_group$AgeGroup,initial_age_group$Veggies)) %>% rename(Age_Group="Var1", Consumes_Veggies = "Var2", Veggie_Consumption_Freq = "Freq")


Healthy_Habits_frequency <- list("PHYS" = PHYS,"FRUIT" = FRUIT,"VEGGIES" = VEG)

Healthy_Habits_frequency 

```


Finally this chuch of code graphs the frequency of poeple who have diabetes based on the healthy habits indicators, grouped by age groups. This would give us a chance to see which poplation of age group held the largest value, based on their health indicators. Are we seeing a consistent pattern in diabetes cases for those above or below the age raanges, or are the health and age factors playing roles in the cases? This series of bar charts might be able to provide insight. 

Below observed the frequencies for whether or not a person has done physical activity in the past 30 days. 

```{r physage, message = FALSE}

initial_age_group_diab <- initial_age_group %>% filter(Diabetes_binary ==1)

phys_sum <- initial_age_group_diab %>% group_by(PhysActivity,AgeGroup) %>% summarize(sum_Diabetes= n())
phys_sum$AgeGroup <- as.factor(phys_sum$AgeGroup)


g <- ggplot(data = phys_sum,mapping=aes(x=AgeGroup ,y= sum_Diabetes, fill = AgeGroup), position = "dodge") +
  geom_bar(stat= "identity" ) +
  facet_wrap(~PhysActivity) + 
  labs(x = "Age Group", y = "Count of Diabetes Cases", title = "Count of Diabetes for Each Physical Activity Indicator by Age Group")
g

```

Finally, this next block of code graphs the frequency of poeple who have diabetes based on their fruit and veggie consumption. For simplicity of this report, below will conbine the Fruits and Veggies variables into one factor based variable. First, we will create a new variable that adds the Fruits and Veggies column. The factor levels will be 0, if there is no consumption at all, 1 if there is consumption in either fruits or veggies, and 2 if there is consumption in both. Then, after summarizing the data by the healthy food indicator and age group, we will produce grouped bar charts of diabetes patients for each indicator by age group. 


```{r eatage, message = FALSE}

initial_age_group_diab$HealthyEating <- as.numeric(as.character(initial_age_group_diab$Fruits)) + as.numeric(as.character(initial_age_group_diab$Veggies))

eat_sum <- initial_age_group_diab %>% group_by(HealthyEating,AgeGroup) %>% summarize(sum_Diabetes= n())
eat_sum$AgeGroup <- as.factor(eat_sum$AgeGroup)

g <- ggplot(data = eat_sum,mapping=aes(x=AgeGroup ,y= sum_Diabetes, fill = AgeGroup), position = "dodge") +
  geom_bar(stat= "identity" ) +
  facet_wrap(~HealthyEating) + 
  labs(x = "Age Group", y = "Count of Diabetes Cases", title = "Count of Diabetes for Each Healthy Eating Indicator by Age Group")
g

```

### Age and Mental Health 

So far we've looked at the intersections of healthy habits, age and diabetes. Finally, it would be interesting to see if mental health, age and diabetes have any intersections. The main point of interest in this portion is to see if there is any possible correlation between age and many days during the past 30 days did the subject say their mental health was not good ( which is the Mental health varaible). To see this, Below looks at a scatter plot of age and mental health broken out by whether or not the subject had diabetes. Is there a stronger correlation for those who have diabetes compared to those who did not? That and other questions that could possibly shed light in the modelling process could be seen in the below set of graphs. 

```{r mentalage}



ment_age <- initial_dta %>% group_by(Diabetes_binary, Age) %>% summarize(mental_health_sum = sum(as.numeric(MentHlth)))

age_mental_scatter <- ggplot(ment_age, aes(x=Age,y=mental_health_sum)) +
  geom_point(position= "jitter") + 
  geom_smooth(method = lm) +
  labs(x = "Ages", y= "Mental Health Days Not God", title= "Age by Number of Reported Days of Mental Health Not Good") +
  facet_wrap(~Diabetes_binary)
  

age_mental_scatter

```

# Summarization and Modelling 

## Log Loss 

When working with data ( such as the diabetes data we are using) , that contians a binary response variable with the possible outcome of 0 or 1 ( occurance or no occurance) one of the main goals in machine learning is to fit a model whose predictions are as close to the actual value as possible. In the statistics world, we want the probability that our predictions are equal to the actual value of the response to be as large possible. Log loss is a metric that uses the probability of obtaining our predicted values to determine how well our model fits the data and forecasts the response. To preovide a view of the mechanics behind the model, we start with the notion that our binary response variable is a random variable that follows a Bernoulli distribution. Thus, to start, we calculate the likelihood of the probability of success for a random variable that follows the Bernoulli distribion with the parameter of p being the probability of success. $$L(p│x_1,…x_n )=∏_np^{x_i}(1-p)^{1-x_i}$$ . After taking the log of the likelihood function, we divide the summation by the number of observations in order to find the average loss for over our sample and multiply by -1 to establish notion that the lowest output value of the log loss is optimal: $$log-loss= -1/n ∑_n x_i logp+(1-x_i ) log⁡(1-p)$$  . Thus, the lower the log-loss, the better our prediction, and the better our model fits the data. The log loss function is often preferred over the accuracy value because it  associated with how close the predictions were to the actual values, as opposed to just how many correct predictions there were out of the total predictions, which is what accuracy focuses on. When we are only observing the proportion of correct predictions as a measure of how well our model fits the data, we could be missing important information on how well the incorrect predictions fared with the actual values. Were they close? was there a large amount of error ? We wouldn't be able to have that kind of visibility when just looking at accuracy, whereas log-loss could give us an idea of how close our predictions are to the actual values, which is an indication of how much error we could be seeing from the target. 

## Logistic Regression 

Logistic regrssion is a modelling technique under the supervised learning umbrella of Machine learning that works with binary data to predict the probability of an event occuring. Similar to linear regression, logistic regression can work to identify the relationship between explanatory variables and the target response variable. However, in the situatio of logistic regression, the response variable is binary and has the value of 0 or 1 as opposed to a regular numeric response that can take on any number- depending on the unit of measurement. To start the breakdown of the model is the following: $$P = e^{B_0 +B_1x}/1+e^{B_0 +B_1*x}$$ where P is the probability of an event occuring and $e^{B_0 +B_1x}$ is obtained by parameter estimation ( mainly from using maximum likelihood), but theoretically, it comes from the log-odds function $Y = ln(P/1-P)$ which calculates the probability of an event happening divided by the probability of the event not happening, where Y can have the components of a lineary regression wihout the erorr term $Y=B_0 +B_1x$ . Thus, if we solve for the probability of an event occuring in terms of the odds that it will occur (Y), then we will get $P=Y/1+Y$ which makes the model the logistic regression. Thus, to sum, we have a categorical variable with the categories of 0 and 1 - event occurs or does not occur- given a set of explanatory variables. Logistic regression can use the explanatory varibles to determine the probability of the response variable occuring. It must be noted that the the linear regrssion portions of the data can be extended to multiple linear regression cases as well. 

### Modelling with Logistic Regression 

Now that we have a good idea oon the foundatoin and theh purpose of logistic regression, let's look at fitting our binary heart disease data to a logistic regression model.

Log- Loss Method Selection Criteria 

Since we have explored the Log-loss criteria for modelling above, this first candidate will use the log-loss criteria to fit a model. Because logistic regression requires all of our variables to binary, we will convert the numeric variables that we have in the data to dummy variables. We will then fit the generalized linear model to the data under the binomial family with the logloss modelling metric. 


```{r logloss, warning = FALSE}
set.seed(10)

newcols <- dummyVars(~BMI + MentHlth + PhysHlth + Age + Education, data = initial_dta_train, stringAsFactors = TRUE, sep = "_") %>% 
  predict(newdata = initial_dta_train)

logdta <- cbind(initial_dta_train,newcols) %>%
  select(-BMI, -MentHlth, - PhysHlth, - Age, - Education, -Education1) 


levels(logdta$Diabetes_binary) = c("yes","no")

logloss <- train(Diabetes_binary ~ ., data =  logdta,
                       method = "glm",
                       family = "binomial",
                       preProcess = c("center", "scale"), 
                       trControl = trainControl(method = "cv", number = 10,classProbs = TRUE, summaryFunction =mnLogLoss),
                      metric = "logLoss"
                 )

summary(logloss)
logloss
```

This next model will use the stepAIC function in the backward direction to find a logistic model, still using the log loss metric. 

```{r logloss2, warning=FALSE}

logloss2 <- train(Diabetes_binary ~ . ,
  data = logdta ,
  method = 'glmStepAIC',
  family = "binomial",
  direction = "backward",
  trControl = trainControl(method = "cv", number = 3,classProbs = TRUE, summaryFunction =mnLogLoss),
  metric = "logLoss", 
  trace = FALSE
  )


summary(logloss2)
logloss2
```

This final model will use the StepAIC fuction in the forward direction where the metric is Accuracy

```{r logloss3, warning=FALSE}

set.seed(10)

lowerfit <- glm(Diabetes_binary ~ 1, data = as.data.frame(logdta), family = binomial)

upperfit <- glm(Diabetes_binary ~ ., data = as.data.frame(logdta), family = binomial)

logloss3 <- stepAIC(lowerfit ,
  data = logdta ,
  family = "binomial",
  direction = "forward",
  trControl = trainControl(method = "cv", number = 3),
  scope=list(upper=upperfit,lower=lowerfit),
  metric = "Accuracy", 
  trace = FALSE
  )


summary(logloss3)
logloss3
```
